{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PNLSTM",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR_w_bmatd1w",
        "colab_type": "code",
        "outputId": "53d94f9d-c7b6-472f-a2a6-ba0f875ec7bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow-gpu\n",
        "!pip install h5py\n",
        "!pip install epitran"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/93/c7bca39b23aae45cd2e85ad3871c81eccc63b9c5276e926511e2e5b0879d/tensorflow_gpu-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 34kB/s \n",
            "\u001b[?25hCollecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 68.8MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.27.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.7.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.21.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (46.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=f8f71599fea26aeb6844c3c8428fd2b850819e1181ba18fbf74555d001415be8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 1.15.2 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.2 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, gast, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2 tensorboard-2.1.1 tensorflow-estimator-2.2.0rc0 tensorflow-gpu-2.1.0\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Collecting epitran\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/ce/ebf70bb24f220c628d4ce1e153c07e95f59132a1d882b586427ade2b1b3d/epitran-1.8-py2.py3-none-any.whl (132kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from epitran) (46.0.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from epitran) (2019.12.20)\n",
            "Collecting panphon>=0.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/25/935f443f0a2cce5d7a4b6b0d9101c990a6f3c74702c02287d4addd3fe009/panphon-0.17-py2.py3-none-any.whl (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[?25hCollecting unicodecsv\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/a4/691ab63b17505a26096608cc309960b5a6bdf39e4ba1a793d5f9b1a53270/unicodecsv-0.14.1.tar.gz\n",
            "Collecting marisa-trie\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/95/d23071d0992dabcb61c948fb118a90683193befc88c23e745b050a29e7db/marisa-trie-0.7.5.tar.gz (270kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 33.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from panphon>=0.16->epitran) (1.18.2)\n",
            "Collecting munkres\n",
            "  Downloading https://files.pythonhosted.org/packages/64/97/61ddc63578870e04db6eb1d3bee58ad4e727f682068a7c7405edb8b2cdeb/munkres-1.1.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from panphon>=0.16->epitran) (3.13)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from panphon>=0.16->epitran) (0.5.3)\n",
            "Building wheels for collected packages: unicodecsv, marisa-trie\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-cp36-none-any.whl size=10768 sha256=7a3d23bb31a29b9dfd923601375bb3e9f3d189441997a19d7e4330c793fc3967\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/09/e9/e800279c98a0a8c94543f3de6c8a562f60e51363ed26e71283\n",
            "  Building wheel for marisa-trie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for marisa-trie: filename=marisa_trie-0.7.5-cp36-cp36m-linux_x86_64.whl size=862357 sha256=e25104d626fe85cb91a4ee9f2102e9cc3a2216c88ba60af10fa671280566df48\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/24/79/022624fc914f0e559fe8a1141aaff1f9df810905a13fc75d57\n",
            "Successfully built unicodecsv marisa-trie\n",
            "Installing collected packages: munkres, unicodecsv, panphon, marisa-trie, epitran\n",
            "Successfully installed epitran-1.8 marisa-trie-0.7.5 munkres-1.1.2 panphon-0.17 unicodecsv-0.14.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTfjZuWS0-Qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential,model_from_json\n",
        "import tensorflow.keras.utils as ku \n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder()\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import choice\n",
        "import string, os \n",
        "import h5py\n",
        "import epitran\n",
        "import math\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1B25AerpEFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "booba = pd.read_csv(\"booba.csv\",index_col=0)\n",
        "damso = pd.read_csv(\"damso.csv\",index_col=0)\n",
        "guizmo = pd.read_csv(\"guizmo.csv\",index_col=0)\n",
        "kaaris = pd.read_csv(\"kaaris.csv\",index_col=0)\n",
        "lomepal = pd.read_csv(\"lomepal.csv\",index_col=0)\n",
        "nekfeu = pd.read_csv(\"nekfeu.csv\",index_col=0)\n",
        "nepal = pd.read_csv(\"nepal.csv\",index_col=0)\n",
        "orelsan = pd.read_csv(\"orelsan.csv\",index_col=0)\n",
        "pnl = pd.read_csv(\"pnl.csv\",index_col=0)\n",
        "sch = pd.read_csv(\"sch.csv\",index_col=0)\n",
        "vald = pd.read_csv(\"vald.csv\",index_col=0)\n",
        "\n",
        "df = booba.append(damso).append(guizmo).append(kaaris).append(lomepal).append(nekfeu).append(nepal).append(orelsan).append(pnl).append(sch).append(vald).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSmLv2Zb0EeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sequence_of_tokens(corpus):\n",
        "    ## tokenization\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "    \n",
        "    ## convert data to sequence of tokens \n",
        "    input_sequences = []\n",
        "    for line in corpus:\n",
        "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(token_list)):\n",
        "            n_gram_sequence = token_list[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "    return input_sequences, total_words, tokenizer\n",
        "\n",
        "def generate_padded_sequences(input_sequences):\n",
        "    max_sequence_len = max([len(x) for x in input_sequences])\n",
        "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "    \n",
        "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "    return predictors, max_sequence_len, label\n",
        "\n",
        "def create_model(max_sequence_len, total_words):\n",
        "    input_len = max_sequence_len - 1\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    # Add Input Embedding Layer\n",
        "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
        "    \n",
        "    model.add(LSTM(128))\n",
        "    \n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "    \n",
        "    return model\n",
        "\n",
        "def make_model(df=df):\n",
        "  sequences = []\n",
        "  for i in range(len(df)):\n",
        "    sequences.extend(df.lyrics_clean[i].replace(\"’\",\" \").replace(\"“\",\" \").split(\"\\n\"))\n",
        "  inp_sequences, total_words, tokenizer = get_sequence_of_tokens(sequences)\n",
        "  predictors, max_sequence_len, label = generate_padded_sequences(inp_sequences)\n",
        "  model = create_model(max_sequence_len, total_words)\n",
        "  \n",
        "  return total_words,max_sequence_len,predictors,label,model,tokenizer\n",
        "\n",
        "def generate_sent(total_words, next_words, model, max_sequence_len, tokenizer, seed_text=\"\"):\n",
        "    seed_text = [seed_text]\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([\" \".join(seed_text)])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predictions = model.predict(token_list, verbose=0).reshape(-1)\n",
        "        predicted = choice(total_words, 2, replace=False, p = predictions)\n",
        "\n",
        "        for word,index in tokenizer.word_index.items():\n",
        "          if index == predicted[0]:\n",
        "            output_word = word\n",
        "            break\n",
        "\n",
        "        try:\n",
        "          if output_word == seed_text[-1] and seed_text[-1] == seed_text[-2]:\n",
        "            for word,index in tokenizer.word_index.items():\n",
        "              if index == predicted[1]:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        except IndexError:\n",
        "          pass\n",
        "\n",
        "        seed_text.append(output_word)\n",
        "    return \" \".join(seed_text)\n",
        "\n",
        "def save_model(model):\n",
        "  model_json = model.to_json()\n",
        "  with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "  model.save_weights(\"model.h5\")\n",
        "  print(\"Saved model to disk\")\n",
        "\n",
        "def load_model(name=\"model.json\",weights=\"model.h5\"):\n",
        "  json_file = open('model.json', 'r')\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "  loaded_model = model_from_json(loaded_model_json)\n",
        "  loaded_model.load_weights(\"model.h5\")\n",
        "  return loaded_model\n",
        "\n",
        "def dotproduct(v1, v2):\n",
        "  return sum((a*b) for a, b in zip(v1, v2))\n",
        "\n",
        "def length(v):\n",
        "  return math.sqrt(dotproduct(v, v))\n",
        "\n",
        "def angle(v1, v2):\n",
        "  return math.acos(dotproduct(v1, v2) / (length(v1) * length(v2)))\n",
        "\n",
        "def matrixer(sequence):\n",
        "  vowels = [\"j\",\"w\",\"ɥ\",\"a\",\"ɑ\",\"e\",\"ɛ\",\"ɛː\",\"ə\",\"i\",\"œ\",\"ø\",\"o\",\"ɔ\",\"u\",\"y\",\"ɑ̃\",\"ɛ̃\",\"œ̃\",\"ɔ̃\"]\n",
        "  phonemes = [['ɑ','a'],['e', 'ɛ', 'ɛː', 'ə'],['i', 'j'],['o','ɔ'],['wa','wɑ','wɛ̃'],['u','w'],['y','ɥ']\n",
        "              ,['ø','œ','e'],['ɔ̃'], ['ɑ̃'], ['ɛ̃','in','œ̃'], ['b'], ['ks','k','kw'],['sj','si']]\n",
        "  vector = list()\n",
        "  rhyme = list()\n",
        "\n",
        "  epi = epitran.Epitran('fra-Latn')\n",
        "\n",
        "  sequence = sequence.lower()\n",
        "  sequence = epi.transliterate(sequence)\n",
        "\n",
        "  for vowel in vowels:\n",
        "    vector.append(sequence.count(vowel))\n",
        "  \n",
        "  for phoneme in phonemes:\n",
        "    somme = 0\n",
        "    for vowel in phoneme:\n",
        "      somme+=sequence.count(vowel)\n",
        "    vector.append(somme)\n",
        "\n",
        "  seq = ''.join([l for l in sequence if l in vowels])\n",
        "  for vowel in vowels:\n",
        "    if seq[-1] == vowel:\n",
        "      rhyme.append(1)\n",
        "    else:\n",
        "      rhyme.append(0)\n",
        "  return [vector,rhyme]\n",
        "\n",
        "def compare(context,sequences):\n",
        "  sequences.insert(0,context)\n",
        "  vectors = list()\n",
        "  rhymes = list()\n",
        "  angles = list()\n",
        "  \n",
        "  for seq in sequences:\n",
        "    vectors.append(matrixer(seq)[0])\n",
        "    rhymes.append(matrixer(seq)[1])\n",
        "  \n",
        "  for i in range(len(vectors)-1):\n",
        "    if rhymes[0] == rhymes[i+1]:\n",
        "      rhyme = 0\n",
        "    else:\n",
        "      rhyme = 1\n",
        "    angles.append(angle(vectors[0],vectors[i+1])+rhyme)\n",
        "  \n",
        "  print(angles)\n",
        "\n",
        "  return sequences[angles.index(min(angles))+1]\n",
        "\n",
        "def couplet(model,max_sequence_len,tokenizer,sents=5,comp=5):\n",
        "  sentlist = [generate_sent(np.random.randint(5,21),model,max_sequence_len,tokenizer)]\n",
        "  for i in range(sents):\n",
        "    context = sentlist[-1]\n",
        "    trials=list()\n",
        "    for i in range(comp):\n",
        "      trials.append(generate_sent(np.random.randint(5,21),model,max_sequence_len,tokenizer))\n",
        "    sentlist.append(compare(context, trials))\n",
        "  return \"\\n\".join(sentlist)\n",
        "\n",
        "def refrain(model, tokenizer, comp=5, form=None):\n",
        "  sentlist = [generate_sent(np.random.randint(5,15),model,max_sequence_len, tokenizer)]\n",
        "  context = sentlist[0]\n",
        "  trials = list()\n",
        "  for j in range(comp):\n",
        "    trials.append(generate_sent(np.random.randint(5,15),model,max_sequence_len,tokenizer))\n",
        "  sentlist.append(compare(context,trials))\n",
        "  if form == None:\n",
        "    form = np.random.randint(0,3)\n",
        "  if form == 0:\n",
        "    return \"\\n\".join([sentlist[0],sentlist[1],sentlist[0],sentlist[1]])\n",
        "  elif form == 1:\n",
        "    return \"\\n\".join([sentlist[0],sentlist[0],sentlist[1],sentlist[1]])\n",
        "  elif form == 2:\n",
        "    return \"\\n\".join([sentlist[0],sentlist[1],sentlist[1],sentlist[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELr31hHfVArg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_words, max_sequence_len, predictors, label, model, tokenizer = make_model(pnl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsjWTOeAyA6X",
        "colab_type": "code",
        "outputId": "82bed2b7-c3b1-4d36-c9cd-b42dede83e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 46, 10)            49100     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               71168     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4910)              633390    \n",
            "=================================================================\n",
            "Total params: 753,658\n",
            "Trainable params: 753,658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NytieibFQgAg",
        "colab_type": "code",
        "outputId": "abb0a164-bb39-4b88-b599-6390ddb30ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x=predictors, y=label, epochs=30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 39033 samples\n",
            "Epoch 1/30\n",
            "39033/39033 [==============================] - 19s 491us/sample - loss: 6.6612\n",
            "Epoch 2/30\n",
            "39033/39033 [==============================] - 11s 282us/sample - loss: 6.2261\n",
            "Epoch 3/30\n",
            "39033/39033 [==============================] - 11s 285us/sample - loss: 5.9658\n",
            "Epoch 4/30\n",
            "39033/39033 [==============================] - 11s 282us/sample - loss: 5.7641\n",
            "Epoch 5/30\n",
            "39033/39033 [==============================] - 11s 283us/sample - loss: 5.5999\n",
            "Epoch 6/30\n",
            "39033/39033 [==============================] - 11s 281us/sample - loss: 5.4572\n",
            "Epoch 7/30\n",
            "39033/39033 [==============================] - 11s 282us/sample - loss: 5.3278\n",
            "Epoch 8/30\n",
            "39033/39033 [==============================] - 11s 284us/sample - loss: 5.2097\n",
            "Epoch 9/30\n",
            "39033/39033 [==============================] - 11s 287us/sample - loss: 5.1012\n",
            "Epoch 10/30\n",
            "39033/39033 [==============================] - 11s 282us/sample - loss: 5.0021\n",
            "Epoch 11/30\n",
            "39033/39033 [==============================] - 11s 283us/sample - loss: 4.9009\n",
            "Epoch 12/30\n",
            "39033/39033 [==============================] - 11s 283us/sample - loss: 4.8080\n",
            "Epoch 13/30\n",
            "39033/39033 [==============================] - 11s 284us/sample - loss: 4.7130\n",
            "Epoch 14/30\n",
            "39033/39033 [==============================] - 11s 283us/sample - loss: 4.6285\n",
            "Epoch 15/30\n",
            "39033/39033 [==============================] - 11s 281us/sample - loss: 4.5479\n",
            "Epoch 16/30\n",
            "39033/39033 [==============================] - 11s 278us/sample - loss: 4.4716\n",
            "Epoch 17/30\n",
            "39033/39033 [==============================] - 11s 281us/sample - loss: 4.4032\n",
            "Epoch 18/30\n",
            "39033/39033 [==============================] - 11s 286us/sample - loss: 4.3405\n",
            "Epoch 19/30\n",
            "39033/39033 [==============================] - 11s 278us/sample - loss: 4.2833\n",
            "Epoch 20/30\n",
            "39033/39033 [==============================] - 11s 278us/sample - loss: 4.2289\n",
            "Epoch 21/30\n",
            "39033/39033 [==============================] - 11s 276us/sample - loss: 4.1772\n",
            "Epoch 22/30\n",
            "39033/39033 [==============================] - 11s 278us/sample - loss: 4.1306\n",
            "Epoch 23/30\n",
            "39033/39033 [==============================] - 11s 278us/sample - loss: 4.0848\n",
            "Epoch 24/30\n",
            "39033/39033 [==============================] - 11s 278us/sample - loss: 4.0415\n",
            "Epoch 25/30\n",
            "39033/39033 [==============================] - 11s 279us/sample - loss: 3.9993\n",
            "Epoch 26/30\n",
            "39033/39033 [==============================] - 11s 277us/sample - loss: 3.9576\n",
            "Epoch 27/30\n",
            "39033/39033 [==============================] - 11s 278us/sample - loss: 3.9202\n",
            "Epoch 28/30\n",
            "39033/39033 [==============================] - 11s 277us/sample - loss: 3.8817\n",
            "Epoch 29/30\n",
            "39033/39033 [==============================] - 11s 274us/sample - loss: 3.8469\n",
            "Epoch 30/30\n",
            "39033/39033 [==============================] - 11s 275us/sample - loss: 3.8133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f43bc524898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRI1Caq3IugP",
        "colab_type": "code",
        "outputId": "dd8aec6f-b528-44f2-ece5-5d53ea2c3674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "generate_sent(total_words, 10, model, max_sequence_len,tokenizer, \"ouais\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ouais plus au fond de la sère l orfèvres pas qu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10coy4fIIMCe",
        "colab_type": "code",
        "outputId": "d3c8f4a2-3272-4553-f1fd-2b9ca7a4750a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print(couplet(model,max_sequence_len,tokenizer))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.2871921089465703, 1.4284525545876783, 1.4853002229910166, 1.880208911174086, 0.6404342992971294]\n",
            "[0.4889340593007351, 1.862201467699339, 1.9823163392199068, 1.7780432857004544, 1.5752903348411684]\n",
            "[1.390720773122277, 1.447255423508109, 1.4482586717402668, 0.651547090712549, 1.663686908743296]\n",
            "[1.9390702522212604, 1.6306585595278458, 1.7043991861118855, 1.7974961179982771, 1.6928592392357809]\n",
            "[1.489453454476958, 1.8089218810405066, 1.7975615252157127, 1.3948803566773706, 1.604895314547286]\n",
            " ton bâtard t court pas la semelle j maille j emmène ce des\n",
            " ça fait pas trop calmant que la famille c\n",
            " l histoire à les fils de d sentiments là pas comme l\n",
            " et j suis pas buraliste j me\n",
            " les anges plus fait l marques tout sur la rue mètres hmm incompréhensibles pas une\n",
            " des talons la gueule sur les escalier je comptes que j\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAh74TMj71tz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}